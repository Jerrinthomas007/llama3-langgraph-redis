# ğŸ§  llama3-redis-qdrant-chat

A **production-ready, scalable AI chatbot** that uses:

- âš¡ [Groq](https://console.groq.com/) + **LLaMA3** for blazing-fast LLM inference
- ğŸ§  **Redis** for short-term chat memory
- ğŸŒ Custom DuckDuckGo HTML search and smart scraping (no third-party DuckDuckGo lib)
- ğŸ“š **LangGraph** for stateful, multi-step chat workflows
- ğŸ’» React + Vite frontend for a clean chat UI

---

## âš™ï¸ Features

- ğŸ” Real-time chat history using Redis
- ğŸŒ Online search using custom HTML scraping from DuckDuckGo
- ğŸ“„ Webpage content extraction using `trafilatura`
- ğŸŒ¦ï¸ Live weather support via DNS Toys
- ğŸ§  LLM-powered reasoning to decide between online search or direct response
- âœ¨ Extensible LangGraph architecture for multi-step chains
- ğŸ’¬ Responsive frontend running on Vite (port `5173`)

---

## ğŸ§  How Online Search Works

This chatbot **does not rely on any DuckDuckGo Python library**.

It uses:

- **BeautifulSoup** to parse DuckDuckGo's HTML results (`https://html.duckduckgo.com/html/?q=...`)
- **Trafilatura** to extract clean text from webpages
- A separate LLM step to select the most relevant link for scraping

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ chatbot.py           # LangGraph state machine
â”‚   â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”‚   â”œâ”€â”€ online_search.py # Custom DuckDuckGo + scraping
â”‚   â”‚   â”‚   â””â”€â”€ dnstoys.py       # Weather info via DNS
â”‚   â”‚   â”œâ”€â”€ core/                # Redis setup
â”‚   â”‚   â””â”€â”€ main.py              # FastAPI entry point (optional)
â”‚   â””â”€â”€ shared/                  # Typed request/response schemas
â”œâ”€â”€ frontend/                    # Vite + React frontend
â”œâ”€â”€ .env.sample                  # Sample env variables
```

---

## ğŸš€ Getting Started

### ğŸ Backend Setup

```bash
# Step 1: Create and activate Python environment
conda create -n llama3-redis-qdrant-chat python=3.10 -y
conda activate llama3-redis-qdrant-chat

# Step 2: Install backend dependencies
pip install -r requirements.txt

# Step 3: Set up environment variables
cp .env.sample .env
# ğŸ‘‰ Edit the new .env file and add your Groq API key:
# GROQ_API_KEY=your_groq_api_key_here

# Step 4: Start the backend
uvicorn src.backend.main:app --reload
```

> Ensure Redis is running locally on port `6379`

---

### ğŸ’» Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

> The frontend will be available at: `http://localhost:5173`

---

## ğŸ§  LangGraph Workflow

1. ğŸ”„ Load user history from Redis
2. ğŸ§  Decide via LLaMA3 if external search is needed
3. ğŸŒ If needed, search via custom DuckDuckGo scraper and pick best result
4. ğŸ•¸ Scrape that result using `trafilatura`
5. ğŸ¤– Generate response using prompt + history + context
6. ğŸ’¾ Store updated history in Redis

---


## ğŸ”§ Environment Setup

Edit `.env` file with your API keys:

```env
GROQ_API_KEY=your_groq_api_key
```

---


## ğŸ¤ Contributing

Got improvements or ideas? PRs and issues are welcome!

---

## ğŸ“œ License

MIT Â© [Jerrin Thomas](https://github.com/yourusername)